{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f7dde9-c672-4452-b7c2-079e375403ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,Row\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28982433-ff2d-4f25-96a2-b955b446fc31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def monToMonthText(m):\n",
    "    if m == 1:\n",
    "        return \"Jan\"\n",
    "    if m == 2:\n",
    "        return \"Feb\"\n",
    "    if m == 3:\n",
    "        return \"Mar\"\n",
    "    if m == 4:\n",
    "        return \"Apr\"\n",
    "    if m == 5:\n",
    "        return \"May\"\n",
    "    if m == 6:\n",
    "        return \"Jun\"\n",
    "    if m == 7:\n",
    "        return \"Jul\"\n",
    "    if m == 8:\n",
    "        return \"Aug\"\n",
    "    if m == 9:\n",
    "        return \"Sept\"\n",
    "    if m == 10:\n",
    "        return \"Oct\"\n",
    "    if m == 11:\n",
    "        return \"Nov\"\n",
    "    if m == 12:\n",
    "        return \"Dec\"\n",
    "\n",
    "def monToQtr(m):\n",
    "    if m >= 1 and m <= 3:\n",
    "        return \"Q1\"\n",
    "    if m >= 4 and m <= 6:\n",
    "        return \"Q2\"\n",
    "    if m >= 7 and m <= 9:\n",
    "        return \"Q3\"\n",
    "    if m >= 10 and m <= 12:\n",
    "        return \"Q4\"\n",
    "\n",
    "def timeOfDay(hour_int):\n",
    "    if hour_int>=6 and hour_int<=10:\n",
    "        tod=\"Morning\"\n",
    "    if hour_int>=11 and hour_int<=14:\n",
    "        tod=\"Mid Day\"\n",
    "    if hour_int>=14 and hour_int<=18:\n",
    "        tod=\"Afternoon\"\n",
    "    if hour_int>=19 and hour_int<=22:\n",
    "        tod=\"Evening\"\n",
    "    return tod\n",
    "\n",
    "getMonth = udf(lambda x: monToMonthText(x), StringType())\n",
    "getQtr = udf(lambda x: monToQtr(x), StringType())\n",
    "getTimeOfDay = udf(lambda x: timeOfDay(x), StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb934798-8acd-4dd0-aa95-d931b20137ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/POS.csv\")\n",
    "df2=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/Inventory.csv\")\n",
    "df3=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/supply_chain_data.csv\")\n",
    "df4=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/Product.csv\")\n",
    "df5=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/Store.csv\")\n",
    "df6=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/Location.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02c6dce8-3550-4598-a426-63582bbff7ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove Rows with Null Values in Certain Columns\n",
    "df11 = df1.na.drop(subset=[\"StoreID\", \"SKU\", \"Quantity\", \"Date\", \"Time\"])\n",
    "df22 = df2.na.drop(subset=[\"SKU\", \"StoreID\", \"SupplierID\"])\n",
    "df33 = df3.na.drop(subset=[\"SKU\", \"StoreID\", \"SupplierID\", \"LocationID\"])\n",
    "df44 = df4.na.drop(subset=[\"SKU\", \"ProductName\", \"Category\"])\n",
    "df55 = df5.na.drop(subset=[\"StoreID\", \"LocationID\", \"Address\"])\n",
    "df66 = df6.na.drop(subset=[\"Location\", \"Region\", \"State\", \"Zip\", \"Country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b5c87d-c2fe-4090-9e2c-f206588f9479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove Duplicate Rows with Certain Columns\n",
    "df111 = df11.dropDuplicates([\"StoreID\", \"SKU\", \"Quantity\", \"Date\", \"Time\"])\n",
    "df222 = df22.dropDuplicates([\"SKU\", \"ProductName\", \"StoreID\", \"SupplierID\"])\n",
    "df333 = df33.dropDuplicates([\"SKU\", \"StoreID\", \"SupplierID\", \"LocationID\"])\n",
    "df444 = df44.dropDuplicates([\"SKU\", \"ProductName\", \"Category\"])\n",
    "df555 = df55.dropDuplicates([\"StoreID\", \"LocationID\", \"Address\"])\n",
    "df666 = df66.dropDuplicates([\"Location\", \"Region\", \"State\", \"Zip\", \"Country\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ce4bdb8-4f22-4e6b-b3d8-b4365a31700f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Normaize Date & Time\n",
    "temp11 = df111.select(\"*\").withColumn ( 'dateInFormat', F.concat ( F.split('Date', '-')[0], F.lit('-'), F.split('Date', '-')[1], F.lit('-'),F.split('Date', '-')[2] ) ) . withColumn('Day', (F.split('Date', '-')[2]).cast('int')).withColumn('Year', F.split(col('Date'), '-')[0]). withColumn('Month', F.split('Date', '-')[1])\n",
    "temp12 = temp11.select(\"*\").withColumn('Qtr', getQtr( col('Month').cast('int'))).withColumn('Mon', getMonth( col('Month').cast('int')))\n",
    "temp13 = temp12.select(\"*\").withColumn('T', (F.split('Time', ' ')[1]))\n",
    "temp14 = temp13.select(\"*\").withColumn('H', (F.split('T', ':')[0]).cast('int'))\n",
    "temp15 = temp14.select(\"*\").withColumn('HourOfDay', getTimeOfDay(col('H').cast('int')))\n",
    "temp16 = temp15.select(\"*\").withColumn('DayOfWeek', F.date_format('dateInFormat', 'E'))\n",
    "df1111 = temp16\n",
    "df111.show(10)\n",
    "#temp11.show(10)\n",
    "#temp12.show(10)\n",
    "temp16.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b99dcff-c624-4433-9112-4023edb9f16e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Enrich POS Data\n",
    "enrichedPOSStep1 = df1111.join(df5, [\"StoreID\"])\n",
    "enrichedPOSStep2 = enrichedPOSStep1.join(df666, [\"LocationID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58a8dd55-ea3f-4f5a-9ff8-51a6be040842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1111.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05689b56-207d-4fcc-945a-932ef66e3721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Enrich SupplyChain\n",
    "enrichedSupplyChain = df333.join(df6, [\"LocationID\"], \"inner\")\n",
    "enrichedSupplyChain1 = enrichedSupplyChain.join(df444, [\"SKU\"], \"inner\").drop(df4[\"Category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac8ef80-0529-413a-9716-8ee55dd3faff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RandomForestRegressor - Inventory Prediction\n",
    "cat_cols = [\"Category\", \"StoreID\", \"SupplierID\", \"LocationID\", \"Region\", \"State\", \"Zip\", \"ProductName\"]\n",
    "stages = []\n",
    "\n",
    "for c in cat_cols:\n",
    "\tstringIndexer = StringIndexer(inputCol=c, outputCol=c + \"_index\")\n",
    "\tencoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],outputCols=[c + \"_vec\"])\n",
    "\tstages += [stringIndexer, encoder]\n",
    "\n",
    "\n",
    "# Transform all features into a vector\n",
    "num_cols = [\"Costs\", \"ProductionVolumes\"]\n",
    "assemblerInputs = [c + \"_vec\" for c in cat_cols] + num_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Create pipeline and use on dataset\n",
    "pipeline = Pipeline(stages=stages)\n",
    "enrichedSupplyChain3 = pipeline.fit(enrichedSupplyChain1).transform(enrichedSupplyChain1)\n",
    "\n",
    "train, test = enrichedSupplyChain3.randomSplit([0.80, 0.20], seed=12345)\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='NumberOfProductsSold')\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "train_predictions = rf_model.transform(train)\n",
    "test_preds = rf_model.transform(test)\n",
    "print(test_preds)\n",
    "\n",
    "#*** NEED TO CORRECT THIS\n",
    "#test_preds.select(\"Category\", \"StoreID\", \"SupplierID\", \"LocationID\", \"Region\", \"State\", \"Zip\", \"ProductName\", \"prediction\").write.mode(\"overwrite\").csv(\"///G://My Drive//Sumit_Consulting//2023//Clients//BMC//Project2//HandsOnProj1//Code//SumitData//RandomForestRegressorInventory_tests_preds.csv\")\n",
    "test_preds.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/tables/RandomForestRegressorInventory_tests_preds.parquet\")\n",
    "\n",
    "def extract_feature_imp(feature_imp, dataset, features_col):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[features_col].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[features_col].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    feature_list = pd.DataFrame(list_extract)\n",
    "    feature_list['score'] = feature_list['idx'].apply(lambda x: feature_imp[x])\n",
    "    return(feature_list.sort_values('score', ascending = False))\n",
    "\n",
    "\n",
    "feature_list = extract_feature_imp(rf_model.featureImportances, train, \"features\")\n",
    "top_20_features = feature_list.sort_values('score', ascending = False).head(20)\n",
    "print(top_20_features)\n",
    "#*** NEED TO CORRECT THIS\n",
    "top_20_features.to_csv('/dbfs/FileStore/tables/RandomForestRegressorInventory_Top20Features.csv', index=False)\n",
    "\n",
    "evaluator1 = RegressionEvaluator(predictionCol=\"prediction\",  labelCol='NumberOfProductsSold', metricName=\"r2\")\n",
    "print(\"Train R2:\", evaluator1.evaluate(train_predictions))\n",
    "print(\"Test R2:\", evaluator1.evaluate(test_preds))\n",
    "\n",
    "\n",
    "evaluator2 = RegressionEvaluator(predictionCol=\"prediction\",  labelCol='NumberOfProductsSold', metricName=\"mae\")\n",
    "print(\"Train mae:\", evaluator2.evaluate(train_predictions))\n",
    "print(\"Test mae:\", evaluator2.evaluate(test_preds))\n",
    "\n",
    "\n",
    "evaluator3 = RegressionEvaluator(predictionCol=\"prediction\",  labelCol='NumberOfProductsSold', metricName=\"rmse\")\n",
    "print(\"Train RMSE:\", evaluator3.evaluate(train_predictions))\n",
    "print(\"Test RMSE:\", evaluator3.evaluate(test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e51577-b6f4-4b23-882f-67731fe5a620",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#RandomForestRegressor - POS Prediction\n",
    "cat_cols = [\"SKU\", \"Category\", \"StoreID\", \"Year\", \"Mon\", \"Qtr\", \"HourOfDay\", \"DayOfWeek\", \"Location\", \"Region\", \"State\", \"Zip\"]\n",
    "stages = []\n",
    "\n",
    "for c in cat_cols:\n",
    "\tstringIndexer = StringIndexer(inputCol=c, outputCol=c + \"_index\")\n",
    "\tencoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],outputCols=[c + \"_vec\"])\n",
    "\tstages += [stringIndexer, encoder]\n",
    "\n",
    "\n",
    "# Transform all features into a vector\n",
    "num_cols = []\n",
    "assemblerInputs = [c + \"_vec\" for c in cat_cols] + num_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Create pipeline and use on dataset\n",
    "pipeline = Pipeline(stages=stages)\n",
    "df1_x = pipeline.fit(enrichedPOSStep2).transform(enrichedPOSStep2)\n",
    "\n",
    "train, test = df1_x.randomSplit([0.80, 0.20], seed=12345)\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol='features', labelCol='Quantity')\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "train_predictions = rf_model.transform(train)\n",
    "\n",
    "test_preds = rf_model.transform(test)\n",
    "\n",
    "print(test_preds)\n",
    "\n",
    "#*** NEED TO CORRECT THIS\n",
    "#test_preds.select(\"Category\", \"StoreID\", \"SupplierID\", \"LocationID\", \"Region\", \"State\", \"Zip\", \"ProductName\", \"prediction\").write.mode(\"overwrite\").csv(\"///G://My Drive//Sumit_Consulting//2023//Clients//BMC//Project2//HandsOnProj1//Code//SumitData//RandomForestRegressorInventory_tests_preds.csv\")\n",
    "test_preds.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/tables/RandomForestRegressorPOS_tests_preds.parquet\")\n",
    "\n",
    "\n",
    "feature_list = extract_feature_imp(rf_model.featureImportances, train, \"features\")\n",
    "top_20_features = feature_list.sort_values('score', ascending = False).head(20)\n",
    "#*** NEED TO CORRECT THIS\n",
    "top_20_features.to_csv('/dbfs/FileStore/tables/RandomForestRegressorPOS_Top20Features.csv', index=False)\n",
    "\n",
    "# Then make your desired plot function to visualize feature importance\n",
    "#plot_feature_importance(top_20_features['score'], top_20_features['name'])\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "evaluator1 = RegressionEvaluator(predictionCol=\"prediction\",  labelCol='Quantity', metricName=\"r2\")\n",
    "print(\"Train R2:\", evaluator1.evaluate(train_predictions))\n",
    "print(\"Test R2:\", evaluator1.evaluate(test_preds))\n",
    "\n",
    "\n",
    "evaluator2 = RegressionEvaluator(predictionCol=\"prediction\",  labelCol='Quantity', metricName=\"mae\")\n",
    "print(\"Train mae:\", evaluator2.evaluate(train_predictions))\n",
    "print(\"Test mae:\", evaluator2.evaluate(test_preds))\n",
    "\n",
    "evaluator3 = RegressionEvaluator(predictionCol=\"prediction\",  labelCol='Quantity', metricName=\"rmse\")\n",
    "print(\"Train RMSE:\", evaluator3.evaluate(train_predictions))\n",
    "print(\"Test RMSE:\", evaluator3.evaluate(test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "710d974a-0591-4cb4-a342-dd66a89a41e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Linear Regression Inventory\n",
    "cat_cols = [\"Category\", \"StoreID\", \"SupplierID\", \"LocationID\", \"Region\", \"State\", \"Zip\", \"ProductName\"]\n",
    "stages = []\n",
    "\n",
    "for c in cat_cols:\n",
    "\tstringIndexer = StringIndexer(inputCol=c, outputCol=c + \"_index\")\n",
    "\tencoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],outputCols=[c + \"_vec\"])\n",
    "\tstages += [stringIndexer, encoder]\n",
    "\n",
    "\n",
    "# Transform all features into a vector\n",
    "num_cols = [\"Costs\", \"ProductionVolumes\"]\n",
    "assemblerInputs = [c + \"_vec\" for c in cat_cols] + num_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Create pipeline and use on dataset\n",
    "pipeline = Pipeline(stages=stages)\n",
    "df3_x = pipeline.fit(enrichedSupplyChain1).transform(enrichedSupplyChain1)\n",
    "\n",
    "\n",
    "train, test = df3_x.randomSplit([0.90, 0.10], seed=1234567)\n",
    "\n",
    "\n",
    "# Fit scaler to train dataset\n",
    "scaler = StandardScaler().setInputCol('features').setOutputCol('scaled_features')\n",
    "scaler_model = scaler.fit(train)\n",
    "\n",
    "# Scale train and test features\n",
    "train = scaler_model.transform(train)\n",
    "test = scaler_model.transform(test)\n",
    "\n",
    "\n",
    "lr = LinearRegression(featuresCol='scaled_features', labelCol='NumberOfProductsSold')\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "train_predictions = lr_model.transform(train)\n",
    "test_predictions = lr_model.transform(test)\n",
    "print(test_predictions)\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"NumberOfProductsSold\", metricName=\"r2\")\n",
    "evaluator1 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"NumberOfProductsSold\", metricName=\"mae\")\n",
    "evaluator2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"NumberOfProductsSold\", metricName=\"rmse\")\n",
    "\n",
    "print(\"Train R2:\", evaluator.evaluate(train_predictions))\n",
    "print(\"Test R2:\", evaluator.evaluate(test_predictions))\n",
    "\n",
    "print(\"Train MAE:\", evaluator1.evaluate(train_predictions))\n",
    "print(\"Test MAE:\", evaluator1.evaluate(test_predictions))\n",
    "\n",
    "print(\"Train RMSE:\", evaluator2.evaluate(train_predictions))\n",
    "print(\"Test RMSE:\", evaluator2.evaluate(test_predictions))\n",
    "\n",
    "\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "\n",
    "list_extract = []\n",
    "for i in df3_x.schema['features'].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "    list_extract = list_extract + df3_x.schema['features'].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "\n",
    "varlist = pd.DataFrame(list_extract)\n",
    "varlist['weight'] = varlist['idx'].apply(lambda x: lr_model.coefficients[x])\n",
    "weights = varlist.sort_values('weight', ascending = False)\n",
    "#*** NEED TO CORRECT THIS\n",
    "weights.to_csv('/dbfs/FileStore/tables/LinearRegressionInventory_Weights.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c88144b-2990-4a29-8969-cbd0f9b54979",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Linear Regression POS\n",
    "cat_cols = [\"SKU\", \"Category\", \"StoreID\", \"Year\", \"Mon\", \"Qtr\", \"HourOfDay\", \"DayOfWeek\", \"Location\", \"Region\", \"State\", \"Zip\"]\n",
    "stages = []\n",
    "\n",
    "for c in cat_cols:\n",
    "\tstringIndexer = StringIndexer(inputCol=c, outputCol=c + \"_index\")\n",
    "\tencoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],outputCols=[c + \"_vec\"])\n",
    "\tstages += [stringIndexer, encoder]\n",
    "\n",
    "\n",
    "# Transform all features into a vector\n",
    "num_cols = [\"Quantity\"]\n",
    "assemblerInputs = [c + \"_vec\" for c in cat_cols] + num_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# Create pipeline and use on dataset\n",
    "pipeline = Pipeline(stages=stages)\n",
    "df3_x = pipeline.fit(enrichedPOSStep2).transform(enrichedPOSStep2)\n",
    "\n",
    "train, test = df3_x.randomSplit([0.90, 0.10], seed=1234567)\n",
    "\n",
    "# Fit scaler to train dataset\n",
    "scaler = StandardScaler().setInputCol('features').setOutputCol('scaled_features')\n",
    "scaler_model = scaler.fit(train)\n",
    "\n",
    "# Scale train and test features\n",
    "train = scaler_model.transform(train)\n",
    "test = scaler_model.transform(test)\n",
    "\n",
    "\n",
    "lr = LinearRegression(featuresCol='scaled_features', labelCol='Quantity')\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "train_predictions = lr_model.transform(train)\n",
    "test_predictions = lr_model.transform(test)\n",
    "print(test_predictions)\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Quantity\", metricName=\"r2\")\n",
    "print(\"Train R2:\", evaluator.evaluate(train_predictions))\n",
    "print(\"Test R2:\", evaluator.evaluate(test_predictions))\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Quantity\", metricName=\"mae\")\n",
    "print(\"Train MAE:\", evaluator.evaluate(train_predictions))\n",
    "print(\"Test MAE:\", evaluator.evaluate(test_predictions))\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "\n",
    "list_extract = []\n",
    "for i in df3_x.schema['features'].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "    list_extract = list_extract + df3_x.schema['features'].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "\n",
    "varlist = pd.DataFrame(list_extract)\n",
    "varlist['weight'] = varlist['idx'].apply(lambda x: lr_model.coefficients[x])\n",
    "weights = varlist.sort_values('weight', ascending = False)\n",
    "\n",
    "#*** NEED TO CORRECT THIS\n",
    "weights.to_csv('/dbfs/FileStore/tables/LinearRegressionPOS_Weights.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "AllCode",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
